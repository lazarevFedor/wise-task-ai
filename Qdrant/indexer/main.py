#!/usr/bin/env python3
"""
Industrial RAG Indexer: –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∏ —Ñ–æ—Ä–º—É–ª.
"""

import os
import time
import argparse
import re
from pathlib import Path
from functools import lru_cache
from typing import List, Dict, Tuple
from dataclasses import dataclass
import uuid

from tqdm import tqdm
from pylatexenc.latex2text import LatexNodes2Text
from embedding import EmbeddingModel
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct


@dataclass
class ChunkConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏"""
    max_chunk_size: int = 3000
    min_chunk_size: int = 200
    overlap_size: int = 100
    structural_priority: bool = True


class IndustrialChunker:
    """–ü—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π —á–∞–Ω–∫–µ—Ä —Å –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π"""
    
    def __init__(self, config: ChunkConfig):
        self.config = config
        # Wiki-like patterns kept for legacy converted sources
        self.structural_patterns = [
            (r'\{\{–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ[^}]*\}(.*?)\}\}', 'definition'),
            (r'\{\{–¢–µ–æ—Ä–µ–º–∞[^}]*\}(.*?)\}\}', 'theorem'),
            (r'\{\{–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ[^}]*\}(.*?)\}\}', 'statement'),
            (r'\{\{–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ[^}]*\}(.*?)\}\}', 'proof'),
            (r'== [^=]+ ==', 'section'),
            (r'=== [^=]+ ===', 'subsection'),
        ]
    
    def chunk_text(self, text: str) -> List[Dict]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏)
        if self.config.structural_priority:
            structural_chunks = self._structural_chunking(text)
            if structural_chunks and self._validate_chunks(structural_chunks):
                return structural_chunks
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
        semantic_chunks = self._semantic_chunking(text)
        return semantic_chunks

    def chunk_latex(self, raw_latex: str, plain_text: str) -> List[Dict]:
        """LaTeX-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ: –ø—ã—Ç–∞–µ—Ç—Å—è –≤—ã–¥–µ–ª—è—Ç—å –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —Å–µ–∫—Ü–∏–∏ –∏–∑ —Å—ã—Ä–æ–≥–æ LaTeX.
        –ü—Ä–∏ –Ω–µ—É–¥–∞—á–µ ‚Äî –æ—Ç–∫–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∫ —Ä–∞–∑–±–∏–µ–Ω–∏—é plain_text.
        """
        try:
            chunks: list[Dict] = []
            # 1) –°–µ–∫—Ü–∏–∏
            section_pattern = re.compile(r'\\(section|chapter)\*?\{([^}]*)\}', flags=re.IGNORECASE)
            subsection_pattern = re.compile(r'\\(subsection|subsubsection)\*?\{([^}]*)\}', flags=re.IGNORECASE)
            # 2) –¢–µ–æ—Ä–µ–º–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            env_names = ['definition', 'theorem', 'lemma', 'proposition', 'corollary', 'proof']
            env_regex = '|'.join(env_names)
            env_pattern = re.compile(r'\\begin\{(' + env_regex + r')\}([\s\S]*?)\\end\{\1\}', flags=re.IGNORECASE)
            # 3) –§–æ—Ä–º—É–ª—ã-–±–ª–æ–∫–∏
            eq_patterns = [
                re.compile(r'\\begin\{equation\}([\s\S]*?)\\end\{equation\}', flags=re.IGNORECASE),
                re.compile(r'\\\[([\s\S]*?)\\\]', flags=re.IGNORECASE),
                re.compile(r'\$\$([\s\S]*?)\$\$', flags=re.IGNORECASE),
            ]

            markers: list[Tuple[int, int, str, str]] = []
            for m in section_pattern.finditer(raw_latex):
                markers.append((m.start(), m.end(), 'section', m.group(0)))
            for m in subsection_pattern.finditer(raw_latex):
                markers.append((m.start(), m.end(), 'subsection', m.group(0)))
            for m in env_pattern.finditer(raw_latex):
                env = m.group(1).lower()
                markers.append((m.start(), m.end(), env, m.group(0)))
            for pat in eq_patterns:
                for m in pat.finditer(raw_latex):
                    markers.append((m.start(), m.end(), 'equation', m.group(0)))

            if not markers:
                # fallback to plain text strategy
                return self.chunk_text(plain_text)

            markers.sort(key=lambda t: t[0])

            last_end = 0
            # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã; –¥–ª–∏–Ω–Ω—ã–µ —Ä–µ–∂–µ–º
            for start, end, kind, content in markers:
                if start > last_end:
                    preceding = raw_latex[last_end:start].strip()
                    if len(preceding) >= self.config.min_chunk_size:
                        chunks.extend(self._split_large_content(preceding, 'content'))
                # —Å–∞–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç
                if len(content) <= self.config.max_chunk_size:
                    chunks.append({'content': content, 'type': kind})
                else:
                    chunks.extend(self._split_large_content(content, kind))
                last_end = end
            if last_end < len(raw_latex):
                tail = raw_latex[last_end:].strip()
                if len(tail) >= self.config.min_chunk_size:
                    chunks.extend(self._split_large_content(tail, 'content'))

            # –í–∞–ª–∏–¥–∞—Ü–∏—è; –µ—Å–ª–∏ –ø–ª–æ—Ö–æ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º plain_text
            if not self._validate_chunks(chunks):
                return self.chunk_text(plain_text)
            return chunks
        except Exception:
            return self.chunk_text(plain_text)
    
    def _structural_chunking(self, text: str) -> List[Dict]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–∞–º"""
        chunks = []
        last_end = 0
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        matches = []
        for pattern, chunk_type in self.structural_patterns:
            for match in re.finditer(pattern, text, re.DOTALL):
                matches.append((match.start(), match.end(), chunk_type, match.group(0)))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏—è–º
        matches.sort(key=lambda x: x[0])
        
        for start, end, chunk_type, content in matches:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–¥ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–º —ç–ª–µ–º–µ–Ω—Ç–æ–º
            if start > last_end:
                preceding = text[last_end:start].strip()
                if len(preceding) >= self.config.min_chunk_size:
                    chunks.extend(self._split_large_content(preceding, 'content'))
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
            if len(content) <= self.config.max_chunk_size:
                chunks.append({'content': content, 'type': chunk_type})
            else:
                # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
                chunks.extend(self._split_large_content(content, chunk_type))
            
            last_end = end
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–π—Å—è —Ç–µ–∫—Å—Ç
        if last_end < len(text):
            remaining = text[last_end:].strip()
            if len(remaining) >= self.config.min_chunk_size:
                chunks.extend(self._split_large_content(remaining, 'content'))
        
        return chunks
    
    def _semantic_chunking(self, text: str) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
            if len(sentence) > self.config.max_chunk_size:
                if current_chunk:
                    chunks.append({'content': current_chunk, 'type': 'content'})
                    current_chunk = ""
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                parts = self._split_long_sentence(sentence)
                chunks.extend([{'content': part, 'type': 'content'} for part in parts])
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—â–∞–µ—Ç—Å—è –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            if current_chunk and len(current_chunk) + len(sentence) > self.config.max_chunk_size:
                chunks.append({'content': current_chunk, 'type': 'content'})
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
                overlap_start = max(0, len(current_chunk) - self.config.overlap_size)
                current_chunk = current_chunk[overlap_start:] + " " + sentence

            else:
                current_chunk += " " + sentence if current_chunk else sentence
        
        if current_chunk:
            chunks.append({'content': current_chunk, 'type': 'content'})
        
        return chunks
    
    def _split_large_content(self, content: str, chunk_type: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞—Å—Ç–∏"""
        # üöÄ –ù–µ —Ä–µ–∑–∞—Ç—å –≤–∞–∂–Ω—ã–µ –±–ª–æ–∫–∏
        if chunk_type in ('definition', 'theorem', 'lemma', 'proposition', 'corollary', 'proof'):
            return [{'content': content, 'type': chunk_type}]
        
        if len(content) <= self.config.max_chunk_size:
            return [{'content': content, 'type': chunk_type}]
        
        if chunk_type != 'content':
            sub_chunks = self._split_by_logical_breaks(content, chunk_type)
            if sub_chunks:
                return sub_chunks
        
        return self._split_fixed_size(content, chunk_type)



    
    def _split_by_logical_breaks(self, content: str, chunk_type: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ –ª–æ–≥–∏—á–µ—Å–∫–∏–º —Ä–∞–∑—Ä—ã–≤–∞–º (—Ç–æ—á–∫–∏, –∑–∞–ø—è—Ç—ã–µ –≤ —Ñ–æ—Ä–º—É–ª–∞—Ö)"""
        chunks = []
        
        # –ò—â–µ–º —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç—ã–º–∏ –∏ —Ç–æ—á–∫–∏ –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Å—Ç–∞ —Ä–∞–∑—Ä—ã–≤–∞
        break_positions = []
        for match in re.finditer(r'[.;]', content):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —á–∞—Å—Ç—å —Ñ–æ—Ä–º—É–ª—ã
            if not self._is_in_math_context(content, match.start()):
                break_positions.append(match.start())
        
        if not break_positions:
            return []
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –ø–æ–∑–∏—Ü–∏—è–º
        start = 0
        for pos in break_positions:
            chunk_content = content[start:pos + 1].strip()
            if (len(chunk_content) >= self.config.min_chunk_size and 
                len(chunk_content) <= self.config.max_chunk_size):
                chunks.append({'content': chunk_content, 'type': chunk_type})
                start = pos + 1
                # –µ—Å–ª–∏ –º–∞–ª–µ–Ω—å–∫–∏–π chunk_type=='content', –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º
        if chunks and chunk_type == 'content' and len(chunks[-1]['content']) < self.config.min_chunk_size:
            chunks[-1]['content'] += " " + chunk_content
        else:
            chunks.append({'content': chunk_content, 'type': chunk_type})

        # –û—Å—Ç–∞–≤—à–∞—è—Å—è —á–∞—Å—Ç—å
        if start < len(content):
            remaining = content[start:].strip()
            if len(remaining) >= self.config.min_chunk_size:
                chunks.append({'content': remaining, 'type': chunk_type})
        
        return chunks
    
    def _split_fixed_size(self, content: str, chunk_type: str) -> List[Dict]:
        """–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
        chunks = []
        start = 0
        
        while start < len(content):
            end = start + self.config.max_chunk_size
            if end >= len(content):
                chunk_content = content[start:].strip()
                if len(chunk_content) >= self.config.min_chunk_size:
                    chunks.append({'content': chunk_content, 'type': chunk_type})
                break
            
            # –ò—â–µ–º —Ö–æ—Ä–æ—à–µ–µ –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞ (–ø—Ä–æ–±–µ–ª –∏–ª–∏ –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
            break_pos = self._find_break_position(content, end)
            chunk_content = content[start:break_pos].strip()
            
            if len(chunk_content) >= self.config.min_chunk_size:
                chunks.append({'content': chunk_content, 'type': chunk_type})
            
            start = break_pos - self.config.overlap_size
        
        return chunks
    
    def _split_long_sentence(self, sentence: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"""
        parts = []
        start = 0
        
        while start < len(sentence):
            end = start + self.config.max_chunk_size
            if end >= len(sentence):
                parts.append(sentence[start:].strip())
                break
            
            # –ò—â–µ–º –º–µ—Å—Ç–æ –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞
            break_pos = self._find_break_position(sentence, end)
            parts.append(sentence[start:break_pos].strip())
            start = break_pos
        
        return parts
    
    def _find_break_position(self, text: str, suggested_pos: int) -> int:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é –¥–ª—è —Ä–∞–∑—Ä—ã–≤–∞ —Ç–µ–∫—Å—Ç–∞"""
        # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        for pos in range(suggested_pos, max(0, suggested_pos - 100), -1):
            if pos < len(text) and text[pos] in '.!?':
                return pos + 1
        
        # –ó–∞—Ç–µ–º –∏—â–µ–º –ø—Ä–æ–±–µ–ª
        for pos in range(suggested_pos, max(0, suggested_pos - 50), -1):
            if pos < len(text) and text[pos].isspace():
                return pos + 1
        
        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º suggested_pos
        return min(suggested_pos, len(text))
    
    def _is_in_math_context(self, text: str, position: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –ø–æ–∑–∏—Ü–∏—è –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤
        left_context = text[max(0, position-10):position]
        right_context = text[position:min(len(text), position+10)]
        
        # –ï—Å–ª–∏ –≤–æ–∫—Ä—É–≥ —Ü–∏—Ñ—Ä—ã, –±—É–∫–≤—ã –∏–ª–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ - –≤–µ—Ä–æ—è—Ç–Ω–æ —Ñ–æ—Ä–º—É–ª–∞
        math_indicators = r'[0-9a-zA-Z\(\)\[\]\+\-\*/=]'
        return (re.search(math_indicators, left_context) and 
                re.search(math_indicators, right_context))
    
    def _validate_chunks(self, chunks: List[Dict]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤"""
        if not chunks:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏—Ö —á–∞–Ω–∫–æ–≤
        for chunk in chunks:
            if len(chunk['content']) > self.config.max_chunk_size * 1.5:
                return False
        
        return True


def wait_for_qdrant(host: str, port: int, timeout: int = 120):
    """–û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Qdrant"""
    client = QdrantClient(host=host, port=port)
    start = time.time()
    while True:
        try:
            client.get_collections()
            return
        except Exception:
            if time.time() - start > timeout:
                raise TimeoutError("Qdrant not ready after waiting")
            time.sleep(1)


def latex_to_text(path: Path) -> tuple[str, str, str]:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç LaTeX –≤ —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫; –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∞–∫–∂–µ —Å—ã—Ä–æ–π LaTeX/—Ç–µ–∫—Å—Ç"""
    data = path.read_bytes()
    
    def _decode_attempt(b: bytes) -> str:
        for enc in ('utf-8', 'cp1251', 'koi8-r'):
            try:
                txt = b.decode(enc)
                if txt.count('ÔøΩ') > 5:
                    continue
                bad_pairs = txt.count('√ê') + txt.count('√ë')
                if enc != 'utf-8' and bad_pairs < 5:
                    return txt
                if enc == 'utf-8' and bad_pairs > 50:
                    continue
                return txt
            except Exception:
                continue
        return b.decode('utf-8', errors='ignore')
    
    raw = _decode_attempt(data)
    suffix = path.suffix.lower()

    if suffix == '.txt':
        text = " ".join(raw.split())
        first_line = raw.splitlines()[0].strip() if raw.splitlines() else path.stem
        title = first_line[:100]
        return text, title, raw

    converter = LatexNodes2Text()
    try:
        text = converter.latex_to_text(raw)
    except Exception:
        text = raw
    
    text = " ".join(text.split())

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    title = ""
    try:
        m = re.search(r"\\(?:section|chapter|title)\*?\{([^}]+)\}", raw)
        if m:
            title = m.group(1).strip()
        else:
            first_line = text.split('\n', 1)[0]
            title = first_line.strip()[:100]
    except Exception:
        title = ""

    return text, title, raw


# –õ–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä
try:
    import pymorphy2
    _lemmatizer = pymorphy2.MorphAnalyzer()
except Exception:
    _lemmatizer = None

_WORD_RE = re.compile(r"[\w']+")


if _lemmatizer:
    @lru_cache(maxsize=100_000)
    def _normalize_word(word: str) -> str:
        try:
            return _lemmatizer.parse(word)[0].normal_form
        except Exception:
            return word
else:
    def _normalize_word(word: str) -> str:
        return word


def lemmatize_text(text: str) -> str:
    """–õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç"""
    if not text:
        return ""
    words = _WORD_RE.findall(text.lower())
    if not _lemmatizer:
        return " ".join(words)
    lemmas = []
    for w in words:
        lemma = _normalize_word(w)
        lemmas.append(lemma)
    return " ".join(lemmas)


def unique_tokens(sequence: str) -> list[str]:
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞"""
    seen: set[str] = set()
    ordered: list[str] = []
    for token in sequence.split():
        if not token or token in seen:
            continue
        seen.add(token)
        ordered.append(token)
    return ordered


def prepare_records_from_file(
    file_path: Path,
    chunker: IndustrialChunker,
    start_id: int = 1,
) -> tuple[list[dict], int]:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–ø–∏—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–º —á–∞–Ω–∫–µ—Ä–æ–º"""
    text, title, raw = latex_to_text(file_path)
    
    # LaTeX-aware chunking first, fallback to semantic/plain
    chunks_data = chunker.chunk_latex(raw, text)
    
    def _sanitize_wiki(text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –≤–∏–∫–∏-—Ä–∞–∑–º–µ—Ç–∫—É, —Å–æ—Ö—Ä–∞–Ω—è—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç"""
        if not text:
            return ''
        # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –º–µ—à–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –º–∞—Ç–µ–º–∞—Ç–∏–∫—É
        text = re.sub(r"\[\[(?:–§–∞–π–ª:|File:)[^\]]*\]\]", " ", text, flags=re.IGNORECASE)
        text = re.sub(r"\[\[(?:[^|\]]*\|)?([^\]]+)\]\]", r"\1", text)
        text = re.sub(r"\{\{[^}]*\}\}", " ", text)
        text = re.sub(r"\|\-+", " ", text)
        text = re.sub(r"\|", " ", text)
        text = " ".join(text.split())
        return text.strip()

    title_clean = _sanitize_wiki(title)
    prefix = (title_clean + ' - ') if title_clean else ''
    title_norm = lemmatize_text(title_clean)
    title_lemmas = unique_tokens(title_norm)
    
    records: list[dict] = []
    for idx, chunk_data in enumerate(chunks_data):
        # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π UUID5 –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞ —á–∞–Ω–∫–∞
        pid = str(uuid.uuid5(uuid.NAMESPACE_URL, f"qdrant::{file_path.name}::{idx}"))
        
        chunk_content = chunk_data['content']
        chunk_type = chunk_data['type']
        
        chunk_clean = _sanitize_wiki(chunk_content)
        text_with_title = prefix + chunk_clean
        
        text_norm = lemmatize_text(text_with_title)
        lemmas = unique_tokens(text_norm)
        
        payload = {
            "source": str(file_path.name),
            "chunk_index": idx,
            "title": title_clean,
            "text": chunk_clean,
            "chunk_text": chunk_clean,
            "text_norm": text_norm,
            "lemmas": lemmas,
            "title_lemmas": title_lemmas,
            "chunk_type": chunk_type,
            "chunk_length": len(chunk_clean)
        }
        
        records.append({
            "id": pid,
            "payload": payload,
            "embed_text": text_with_title,
        })
    
    next_id = start_id + len(records)
    return records, next_id


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data-dir', default='/data/latex_books', help='–ü–∞–ø–∫–∞ —Å .tex —Ñ–∞–π–ª–∞–º–∏')
    parser.add_argument('--recreate', action='store_true', help='–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é')
    parser.add_argument('--batch-size', type=int, default=32, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞')
    parser.add_argument('--warmup', action='store_true', help='–ü—Ä–æ–≥—Ä–µ–≤ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π')
    parser.add_argument('--max-files', type=int, default=None, help='–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤')
    parser.add_argument('--extra-dir', default=os.environ.get('EXTRA_LATEX_DIR', '/data/latex_sources'))
    parser.add_argument('--chunk-max', type=int, default=None, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö')
    parser.add_argument('--chunk-min', type=int, default=None, help='–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö')
    parser.add_argument('--chunk-overlap', type=int, default=None, help='–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤ –≤ —Å–∏–º–≤–æ–ª–∞—Ö –ø—Ä–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º —Ä–∞–∑–±–∏–µ–Ω–∏–∏')
    parser.add_argument('--no-struct-priority', action='store_true', help='–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è')
    args = parser.parse_args()

    env_max = os.environ.get('CHUNK_MAX_LEN')
    env_min = os.environ.get('CHUNK_MIN_LEN')
    env_overlap = os.environ.get('CHUNK_OVERLAP')

    max_chunk_size = args.chunk_max if args.chunk_max is not None else (int(env_max) if env_max else 5000)
    min_chunk_size = args.chunk_min if args.chunk_min is not None else (int(env_min) if env_min else 200)
    overlap_size = args.chunk_overlap if args.chunk_overlap is not None else (int(env_overlap) if env_overlap else 200)
    structural_priority = not args.no_struct_priority

    if min_chunk_size > max_chunk_size:
        print(f"[warn] min_chunk_size {min_chunk_size} > max_chunk_size {max_chunk_size}, –∏—Å–ø—Ä–∞–≤–ª—è—é")
        min_chunk_size = max_chunk_size // 4
    if overlap_size >= max_chunk_size:
        overlap_size = max_chunk_size // 5

    print(f"Chunk config: max={max_chunk_size} min={min_chunk_size} overlap={overlap_size} structural_priority={structural_priority}")

    chunk_config = ChunkConfig(
        max_chunk_size=max_chunk_size,
        min_chunk_size=min_chunk_size,
        overlap_size=overlap_size,
        structural_priority=structural_priority,
    )

    chunker = IndustrialChunker(chunk_config)

    qdrant_host = os.environ.get('QDRANT_HOST', 'qdrant')
    qdrant_port = int(os.environ.get('QDRANT_PORT', '6333'))
    collection_name = os.environ.get('COLLECTION_NAME', 'latex_books')
    model_name = os.environ.get('EMBEDDING_MODEL', 'intfloat/multilingual-e5-small')

    print(f"Waiting for Qdrant at {qdrant_host}:{qdrant_port}...")
    wait_for_qdrant(qdrant_host, qdrant_port)

    print(f"Loading embedding model: {model_name}")
    model = EmbeddingModel(model_name)

    if args.warmup:
        try:
            _ = model.encode(['warmup'])
            print('Model warmup complete.')
        except Exception as e:
            print('Warmup failed:', e)
    
    dim = model.dimension

    def embed_fn(texts: List[str]) -> List[List[float]]:
        return [list(map(float, v)) for v in model.encode(texts)]

    if dim is None:
        raise RuntimeError("Failed to determine embedding dimension")

    client = QdrantClient(host=qdrant_host, port=qdrant_port, prefer_grpc=False)

    if args.recreate:
        print(f"(Re)creating collection '{collection_name}' with dim={dim}")
        try:
            client.recreate_collection(
                collection_name=collection_name, 
                vectors_config=VectorParams(size=dim, distance=Distance.COSINE)
            )
        except Exception as e:
            print("Failed to recreate collection:", e)
            raise
    else:
        try:
            client.get_collection(collection_name)
            print(f"Collection '{collection_name}' exists")
        except Exception:
            print(f"Creating collection '{collection_name}'")
            client.recreate_collection(
                collection_name=collection_name, 
                vectors_config=VectorParams(size=dim, distance=Distance.COSINE)
            )

    data_dir = Path(args.data_dir)
    if not data_dir.exists():
        print(f"Data dir {data_dir} not found; nothing to index.")
        return

    candidates = []
    seen_paths: set[Path] = set()

    def collect_from(folder: Path):
        if not folder.exists():
            return
        exts = ('.tex', '.txt')
        for ext in exts:
            for file_path in folder.rglob(f'*{ext}'):
                try:
                    real = file_path.resolve()
                except Exception:
                    real = file_path
                if real in seen_paths:
                    continue
                seen_paths.add(real)
                candidates.append(file_path)

    collect_from(data_dir)
    extra_dir = Path(args.extra_dir) if args.extra_dir else None
    if extra_dir and extra_dir != data_dir:
        collect_from(extra_dir)

    if not candidates:
        print("No .tex files found in", data_dir)
        return

    candidates.sort()
    if args.max_files is not None:
        candidates = candidates[:args.max_files]

    total = 0
    next_id = 1
    embed_inputs: list[str] = []
    record_buffer: list[dict] = []

    def flush_batch():
        nonlocal total
        if not embed_inputs:
            return
        vectors = embed_fn(embed_inputs)
        points = [
            PointStruct(
                id=rec['id'],
                vector=list(map(float, vec)),
                payload=rec['payload'],
            )
            for rec, vec in zip(record_buffer, vectors)
        ]
        client.upsert(collection_name=collection_name, points=points, wait=True)
        total += len(points)
        embed_inputs.clear()
        record_buffer.clear()

    print(f"Processing {len(candidates)} files with industrial chunker...")
    for path in tqdm(candidates, desc='Indexing files'):
        records, next_id = prepare_records_from_file(path, chunker, start_id=next_id)
        for rec in records:
            embed_inputs.append(rec['embed_text'])
            record_buffer.append(rec)
            if len(embed_inputs) >= args.batch_size:
                flush_batch()

    flush_batch()

    print(f"Indexing complete. Total chunks: {total}")
    try:
        import requests as _rq
        r = _rq.get(f"http://{qdrant_host}:{qdrant_port}/collections/{collection_name}", timeout=5)
        r.raise_for_status()
        js = r.json()
        result = js.get('result', {})
        actual_count = result.get('points_count')
        print(f"Verification: Qdrant has {actual_count} points in '{collection_name}' (expected ~{total})")
    except Exception as e:
        print(f"Verification (HTTP) failed: {e}")


if __name__ == '__main__':
    main()